{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Import_Logfiles_ToBigQuery.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisGutknecht/Google-Search-Console-bulk-query-to-GBQ/blob/master/Import_Logfiles_ToBigQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q936wtqDhnm",
        "colab_type": "text"
      },
      "source": [
        "### 1. Access Logfile data with credentials\n",
        "- See: https://jira.bergzeit.de/browse/BWEB-1866\n",
        "- Directory: https://admin.bergzeit.de/googlebot-logshare/\n",
        "- Credentials\n",
        "\n",
        "> - benutzer: googlebot-logs\n",
        "> - password: |&]H;.S;K%#ymTn0\n",
        "\n",
        "- Comments\n",
        "> Die Dateien sind im CSV-Format, Trenner ist ; und die einzelnen Werte sind mit \" \" umschlossen. Die Dateien stammen (ab dem 29.11.) immer von 23:59, was zur Folge hat, dass jeweils eine Minute dem Folgetag zugerechnet wird. Die IPs sind alle per PTR + A-Record geprüft und stammen somit NUR von Google.\n",
        "\n",
        "- Docs: https://2.python-requests.org/en/v1.1.0/user/authentication/\n",
        "\n",
        "- Example File: https://admin.bergzeit.de/googlebot-logshare/2019-11-27.tgz\n",
        "- Use time package to get current date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8jJmOFKBr0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8810cc32-cd21-4c4a-dd02-df9ee156eaf6"
      },
      "source": [
        "import requests \n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import io\n",
        "import tarfile\n",
        "import time\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime, timedelta\n",
        "from google.cloud import bigquery\n",
        "\n",
        "############# CONFIG START #############\n",
        "\n",
        "# BigQuery constants\n",
        "dataset_id = 'CrawlLogfileData'\n",
        "table_name_current = 'DailyCrawlData_All'\n",
        "project_id = 'bergzeit'\n",
        "\n",
        "# Logfile Server HTTP Auth credentials\n",
        "user = 'googlebot-logs'\n",
        "key = '|&]H;.S;K%#ymTn0'\n",
        "\n",
        "############# CONFIG END ###############\n",
        "\n",
        "\n",
        "## In Cloud Functions, set request as the argument >> def runLogfileDataSave(request):\n",
        "def runLogfileDataSave(): # (request)\n",
        "    logfileFetcher = LogfileFetcher(user, key)\n",
        "    dateYesterday = datetime.strftime(datetime.now() - timedelta(2), '%Y-%m-%d')\n",
        "\n",
        "    logfileData = logfileFetcher.getDataByDate(dateYesterday)\n",
        "    saveLogfilesToDb(logfileData)\n",
        "\n",
        "\n",
        "###################################\n",
        "\n",
        "\n",
        "# Service to fetch logfile crawl content\n",
        "class LogfileFetcher:\n",
        "    def __init__(self, user, key):\n",
        "        self.user = user\n",
        "        self.key = key\n",
        "        \n",
        "    def getDataByDate(self, date):\n",
        "        url = 'https://admin.bergzeit.de/googlebot-logshare/' + str(date) + '.tgz'\n",
        "        response = requests.get(url, auth=(self.user, self.key), stream=True)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            with open('/content/file.tar.gz', 'wb') as file:\n",
        "                file.write(response.raw.read())\n",
        "\n",
        "        # Unzip folder and extract content\n",
        "        tar = tarfile.open(\"/content/file.tar.gz\", \"r:gz\")\n",
        "        logFileContent = []\n",
        "        headerRow = ['Remote IP', 'Request Time', 'User Agent', 'Request', 'Status Code', 'Referer', '', 'Domain']\n",
        "        logFileContent.append(headerRow)\n",
        "\n",
        "        for tarinfo in tar:\n",
        "            domain = str(tarinfo.name).split('-accesslog')[0].split('googlebot-')[1].split('-')[0]\n",
        "            csv_file = io.StringIO(tar.extractfile(tarinfo).read().decode('ascii'))\n",
        "            iterRows = iter(csv.reader(csv_file, delimiter=';'))\n",
        "            next(iterRows)\n",
        "\n",
        "            for row in iterRows:\n",
        "                listRow = row\n",
        "                # Add domain from filename\n",
        "                listRow.append(domain)\n",
        "                logFileContent.append(listRow)\n",
        "\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "\n",
        "        # prepare dataframe\n",
        "        df = pd.DataFrame.from_records(logFileContent)\n",
        "        df.columns = df.iloc[0]\n",
        "        df = df.drop(df.index[0])\n",
        "        df.rename(columns={\n",
        "            'Remote IP': 'BotIpAddress','Request Time': 'RequestTime',\n",
        "            'User Agent': 'UserAgent', 'Request' : 'RequestUrl', 'Status Code' : 'StatusCode'\n",
        "            }, inplace=True)\n",
        "        \n",
        "        df['Referer'] = df.apply (lambda row: self.createReferer(row) , axis=1)\n",
        "        df['RequestUrl'] = df.apply (lambda row: self.createUrl(row)  ,axis=1)\n",
        "        df['RequestTime'] = df.apply (lambda row: self.convertToDateTime(row) , axis=1)\n",
        "        df['Date'] = df.apply (lambda row: self.getDate(row) , axis=1)\n",
        "\n",
        "        df = df[['Date', 'Domain', 'RequestTime', 'RequestUrl', 'UserAgent', 'StatusCode', 'Referer', 'BotIpAddress' ]]\n",
        "        print(df.head())\n",
        "\n",
        "        #Convert dataframe to list of lists\n",
        "        return df.values.tolist()   # to_json(orient='records')\n",
        "        \n",
        "    def createUrl(self, row):\n",
        "        return row.Domain + row.RequestUrl.replace('GET ', '').replace(' HTTP/1.1', '')\n",
        "    \n",
        "    def createReferer(self, row):\n",
        "        return row.Domain + row.Referer.replace('GET ', '').replace(' HTTP/1.1', '')\n",
        "\n",
        "    def convertToDateTime(self, row):\n",
        "        datetime = parse(row.RequestTime.replace(':',' ',1))\n",
        "        return  datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        # return  time.strptime(row.RequestTime, \"%d/%b/%y:%H:%M:%S %z\").strftime(\"%Y-%m-%d %H:%M:%S %z\")\t\n",
        "\n",
        "    def getDate(self, row):\n",
        "        datetime = parse(row.RequestTime)\n",
        "        return  datetime.strftime(\"%Y-%m-%d\")\n",
        "        # return  time.strptime(row.RequestTime,\"%Y-%m-%d %H:%M:%S %z\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "class BigQueryConnector:\n",
        "    def __init__(self, dataset_id, table_name, schema):\n",
        "        self.schema = schema\n",
        " \n",
        "        # Instantiates a client\n",
        "        self.bigquery_client = bigquery.Client(project=project_id)\n",
        " \n",
        "        # Prepares a reference to the new dataset\n",
        "        dataset_ref = self.bigquery_client.dataset(dataset_id)\n",
        "        dataset_ref = bigquery.Dataset(dataset_ref)\n",
        "        dataset = None\n",
        "        try:\n",
        "            dataset = self.bigquery_client.get_dataset(dataset_ref)\n",
        "        except Exception as e:\n",
        "            raise e\n",
        " \n",
        "        table_ref = dataset.table(table_name)\n",
        "        self.table = None\n",
        "        try:\n",
        "            self.table = self.bigquery_client.get_table(table_ref)\n",
        "        except Exception as e:\n",
        "            raise e\n",
        " \n",
        "    def writeRowsToStorage(self, rows):\n",
        "        errors = \"\"\n",
        "        valueList = tuple(rows) # .values()\n",
        "        row = [valueList]\n",
        "\n",
        "        try:\n",
        "            errors = self.bigquery_client.insert_rows(self.table, row)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        " \n",
        "        if len(errors) > 0:\n",
        "            print(errors)\n",
        " \n",
        " \n",
        "def saveLogfilesToDb(data):\n",
        "    values = data\n",
        "\n",
        "    currentSchema_raw = [\n",
        "        bigquery.SchemaField('Date', 'DATE'),\n",
        "        bigquery.SchemaField('Domain', 'STRING'),\n",
        "        bigquery.SchemaField('RequestTime', 'TIMESTAMP'),\n",
        "        bigquery.SchemaField('RequestUrl', 'STRING'),\n",
        "        bigquery.SchemaField('UserAgent', 'STRING'),\n",
        "        bigquery.SchemaField('StatusCode', 'STRING'),\n",
        "        bigquery.SchemaField('Referer', 'STRING'),\n",
        "        bigquery.SchemaField('BotIpAddress', 'STRING')\n",
        "    ]\n",
        "\n",
        "    bq_connector = BigQueryConnector(dataset_id=dataset_id, table_name=table_name_current, schema=currentSchema_raw)\n",
        "\n",
        "    for value in values:\n",
        "        bq_connector.writeRowsToStorage(value)  # json.loads()\n",
        "    print(\"Done with data handling...\")\n",
        "\n",
        "runLogfileDataSave()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        Date  ...  BotIpAddress\n",
            "1  2019-12-06  ...  66.249.76.91\n",
            "2  2019-12-06  ...  66.249.76.64\n",
            "3  2019-12-06  ...  66.249.76.75\n",
            "4  2019-12-06  ...  66.249.76.64\n",
            "5  2019-12-06  ...  66.249.76.75\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy1FO-fgEj1t",
        "colab_type": "text"
      },
      "source": [
        "### Reference: Save to weahter data to BigQuery\n",
        "\n",
        "- Set schema, set dataset and table in BQ\n",
        "\n",
        "- See weather data cloud function: https://colab.research.google.com/drive/1ERqJkvnYS-v10eLes1voh2iIl6gwsqBO#scrollTo=eVAIBSG7VKeK\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kfHoByNEkDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Current weather: http://api.openweathermap.org/data/2.5/weather?q=Otterfing,de&units=metric&appid=bdbb4a147eeeb8717fc10be6e62cbabe\n",
        "# Forecast: http://api.openweathermap.org/data/2.5/forecast?q=Otterfing,de&units=metric&appid=bdbb4a147eeeb8717fc10be6e62cbabe\n",
        "# Cloud Function Project: https://console.cloud.google.com/functions/list?project=adsdataprediction\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "\n",
        "\n",
        "#  Reference: https://googleapis.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.table.Table.html\n",
        "# Imports the Google Cloud client library\n",
        "from google.cloud import bigquery\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.cloud import bigquery\n",
        "\n",
        "############# CONFIG START #############\n",
        "\n",
        "api_key_owp = 'bdbb4a147eeeb8717fc10be6e62cbabe'\n",
        "dataset_id = 'weatherData'\n",
        "table_name_current = 'weatherData_Current'\n",
        "table_name_forecast = 'weatherData_forecasted'\n",
        "project_id = 'yourProjectId'\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "currentSchema_raw = [\n",
        "    bigquery.SchemaField('date', 'DATE'),\n",
        "    bigquery.SchemaField('country', 'STRING'),\n",
        "    bigquery.SchemaField('town_name', 'STRING'),\n",
        "    bigquery.SchemaField('town_id', 'STRING'),\n",
        "    bigquery.SchemaField('currentTime', 'DATETIME'),\n",
        "    bigquery.SchemaField('timeZone', 'STRING'),\n",
        "    bigquery.SchemaField('timeOfDay', 'STRING'),\n",
        "    bigquery.SchemaField('weekday_Index', 'STRING'),\n",
        "    bigquery.SchemaField('weekday_Name', 'STRING'),\n",
        "    bigquery.SchemaField('temperature', 'FLOAT'),\n",
        "    bigquery.SchemaField('overallWeather', 'STRING'),\n",
        "    bigquery.SchemaField('overallWeather_code', 'STRING'),\n",
        "    bigquery.SchemaField('rained', 'BOOL'),\n",
        "    bigquery.SchemaField('rainAmount_3hrs_mm', 'STRING'),\n",
        "    bigquery.SchemaField('humidity', 'INTEGER')\n",
        "]\n",
        "\n",
        "towns = ['Hamburg,de', 'Berlin,de', 'Hannover,de','Bremen,de', 'Dresden,de', 'Leipzig,de'\n",
        "    'Dortmund,de', 'Köln,de', 'Frankfurt,de', 'Nürnberg,de', 'Stuttgart,de', 'München,de', 'Passau,de', 'Konstanz,de',\n",
        "    'Wien,at', 'Linz,at', 'Graz,at', 'Salzburg,at', 'Klagenfurt,at', 'Liezen,at', \n",
        "    'Lienz,at', 'Flachau,at', 'Kufstein, at', 'Innsbruck,at', 'Bregenz,at', 'Feldkirch,at', 'Imst,at', \n",
        "    'Lausanne,ch', 'Bern,ch', 'Martigny,ch', 'Basel,ch', 'Zürich,ch', 'Luzern,ch', 'Chur,ch', \n",
        "    'Sankt Moritz,ch', 'Sankt Gallen,ch', 'Bellinzona,ch', 'Andermatt,ch'\n",
        "    ]\n",
        "\n",
        " \n",
        "############# CONFIG END ###############\n",
        "\n",
        "## In Cloud Functions, set request as the argument >> def runWeatherDataSave(request):\n",
        "def runWeatherDataSave(/*request*/): \n",
        "    weatherFetcher = WeatherFetcher(api_key_owp, towns)\n",
        "    currentWeatherData = weatherFetcher.getCurrentWeatherData()\n",
        "    weatherDataWriter(currentWeatherData)\n",
        "\n",
        "########\n",
        "\n",
        "class BigQueryConnector:\n",
        "    def __init__(self, dataset_id, table_name, schema):\n",
        "        self.schema = schema\n",
        " \n",
        "        # Instantiates a client\n",
        "        self.bigquery_client = bigquery.Client(project=project_id)\n",
        " \n",
        "        # Prepares a reference to the new dataset\n",
        "        dataset_ref = self.bigquery_client.dataset(dataset_id)\n",
        "        dataset_ref = bigquery.Dataset(dataset_ref)\n",
        "        dataset = None\n",
        "        try:\n",
        "            dataset = self.bigquery_client.get_dataset(dataset_ref)\n",
        "        except Exception as e:\n",
        "            if dataset is None:\n",
        "                # Creates the new dataset\n",
        "                dataset = self.bigquery_client.create_dataset(dataset_ref)\n",
        "                print('Dataset {} created.'.format(dataset.dataset_id))\n",
        "            else:\n",
        "                raise e\n",
        " \n",
        "        table_ref = dataset.table(table_name)\n",
        "        self.table = None\n",
        "        try:\n",
        "            self.table = self.bigquery_client.get_table(table_ref)\n",
        "        except Exception as e:\n",
        "            if self.table is None:\n",
        "                self.table = bigquery.Table(table_ref, schema=self.schema)\n",
        "                self.table = self.bigquery_client.create_table(self.table)\n",
        "            else:\n",
        "                raise e\n",
        " \n",
        "    def writeRowsToStorage(self, rows):\n",
        "        errors = \"\"\n",
        "        valueList = tuple(rows.values())\n",
        "        row = [valueList]\n",
        "\n",
        "        try:\n",
        "            errors = self.bigquery_client.insert_rows(self.table, row)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        " \n",
        "        if len(errors) > 0:\n",
        "            print(errors)\n",
        " \n",
        " \n",
        "def weatherDataWriter(data):\n",
        "\n",
        "    values = data\n",
        "    bq_connector = BigQueryConnector(dataset_id=dataset_id, table_name=table_name_current, schema=currentSchema_raw)\n",
        "\n",
        "    for value in values:\n",
        "        bq_connector.writeRowsToStorage(value)\n",
        "    print(\"Done with data handling...\")\n",
        " \n",
        " \n",
        "class WeatherFetcher:\n",
        "    def __init__(self, api_key, towns):\n",
        "        self.api_key = api_key\n",
        "        self.towns = towns\n",
        "        \n",
        "    def getTimeOfDay(self):\n",
        "        now = datetime.now().astimezone(timezone('Europe/Berlin'))\n",
        "        hourOfDay_str = now.strftime(\"%Y-%m-%d %H:%M:%S\").split(' ')[1].split(':')[0]\n",
        "        timeOfDay = 'unknown'\n",
        "        hourOfDay = float(hourOfDay_str)\n",
        "\n",
        "        if(hourOfDay < 7): timeOfDay = 'night'\n",
        "        if(hourOfDay >= 7 and hourOfDay < 11): timeOfDay = 'morning'\n",
        "        if(hourOfDay >= 11 and hourOfDay < 14): timeOfDay = 'noon'\n",
        "        if(hourOfDay >= 14 and hourOfDay < 18): timeOfDay = 'afternoon'\n",
        "        if(hourOfDay >= 18 and hourOfDay < 22): timeOfDay = 'evening'\n",
        "        if(hourOfDay >= 22): timeOfDay = 'night'\n",
        "\n",
        "        return timeOfDay\n",
        " \n",
        "    def getCurrentWeatherData(self):\n",
        "\n",
        "        print(\"Fetching data by town\")\n",
        "        allWeatherData = []\n",
        "        for town in self.towns:\n",
        "            request_url = 'http://api.openweathermap.org/data/2.5/weather?q=' + town + '&units=metric&appid=' + self.api_key\n",
        "            request_json = json.loads(requests.get(request_url).text)\n",
        " \n",
        "            now = datetime.now().astimezone(timezone('Europe/Berlin'))\n",
        "            timeNow = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            weekdayIndex = str(datetime.today().weekday()+1)\n",
        "            weekdayLookup = { '1':'Monday', '2':'Tuesday','3':'Wednesday', '4':'Thursday', '5':'Friday', '6':'Saturday', '7':'Sunday' }\n",
        "\n",
        "            rained = True if 'rain' in request_json else False;\n",
        "            rainAmount_3hrs_mm = request_json['rain']['3h'] if 'rain' in request_json else 0;\n",
        "\n",
        "            townWeather = {\n",
        "                'date' : datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "                'country' : request_json['sys']['country'],\n",
        "                'town_name' : request_json['name'],\n",
        "                'town_id' : str(request_json['id']),\n",
        "                'currentTime' : timeNow,\n",
        "                'timeZone' :  'Europe/Berlin',\n",
        "                'timeOfDay' : self.getTimeOfDay(),\n",
        "                'weekDay_Index' : weekdayIndex,\n",
        "                'weekDay_Name' : weekdayLookup[weekdayIndex],\n",
        "                'temperature' : request_json['main']['temp'],\n",
        "                'overallWeather' : request_json['weather'][0]['main'],\n",
        "                'overallWeather_code' : request_json['weather'][0]['id'],\n",
        "                'rained' : rained,\n",
        "                'rainAmount_3hrs_mm' : rainAmount_3hrs_mm,\n",
        "                'humidity' : request_json['main']['humidity']\n",
        "            }\n",
        "\n",
        "            allWeatherData.append(townWeather)\n",
        "\n",
        "        return allWeatherData\n",
        "\n",
        "######\n",
        "\n",
        "runWeatherDataSave()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}